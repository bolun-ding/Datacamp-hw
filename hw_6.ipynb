#Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_absolute_error, mean_squared_error

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor


import warnings
warnings.filterwarnings('ignore')
pd.set_option("display.max_columns", 101)

data = pd.read_csv("train.csv")
data = data.drop(columns=['id', 'timestamp','country'])

data.loc[data['hours_per_week'].isna(), 'hours_per_week'] = data['hours_per_week'].median()
data.loc[data['telecommute_days_per_week'].isna(), 'telecommute_days_per_week'] = data['telecommute_days_per_week'].median()
data = data.dropna()


data_train = data.copy()
data_train.head()

cat_cols = [c for c in data_train.columns if data_train[c].dtype == 'object' 
            and c not in ['is_manager', 'certifications']]
cat_data = data_train[cat_cols]
cat_cols

binary_cols = ['is_manager', 'certifications']
for c in binary_cols:
    data_train[c] = data_train[c].replace(to_replace=['Yes'], value=1)
    data_train[c] = data_train[c].replace(to_replace=['No'], value=0)

final_data = pd.get_dummies(data_train, columns=cat_cols, drop_first= True)
final_data.shape

y = final_data['salary']
X = final_data.drop(columns=['salary'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print("Training Set Dimensions:", X_train.shape)
print("Validation Set Dimensions:", X_test.shape)

num_cols = ['job_years','hours_per_week','telecommute_days_per_week']
num_cols
scaler = StandardScaler()
scaler.fit(X_train[num_cols])
X_train[num_cols] = scaler.transform(X_train[num_cols])

reg=LinearRegression()
reg.fit(X_train, y_train)

mean_absolute_error(y_train,reg.predict(X_train))
mean_squared_error(y_train,reg.predict(X_train))**0.5

#ridge and lasso regression
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

y = final_data['salary']
X = final_data.drop(columns=['salary'])
# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Ridge and Lasso regressors
# You can adjust the alpha value to perform regularization strength tuning
ridge = Ridge(alpha=1.0)
lasso = Lasso(alpha=0.1)

# Fit the models to the training data
ridge.fit(X_train, y_train)
lasso.fit(X_train, y_train)

# Predict on the test data
ridge_predictions = ridge.predict(X_test)
lasso_predictions = lasso.predict(X_test)

# Compute metrics
ridge_mse = mean_squared_error(y_test, ridge_predictions)
ridge_r2 = r2_score(y_test, ridge_predictions)
ridge_mae = mean_absolute_error(y_test, ridge_predictions)

lasso_mse = mean_squared_error(y_test, lasso_predictions)
lasso_r2 = r2_score(y_test, lasso_predictions)
lasso_mae = mean_absolute_error(y_test, lasso_predictions)

# Print the metrics
print("Ridge Regression Metrics:")
print("Mean Squared Error: ", ridge_mse)
print("R^2 Score: ", ridge_r2)
print("Mean Absolute Error: ", ridge_mae)

print("\nLasso Regression Metrics:")
print("Mean Squared Error: ", lasso_mse)
print("R^2 Score: ", lasso_r2)
print("Mean Absolute Error: ", lasso_mae)
